# Question Answering System using NLP

## Introduction
This project implements a Question Answering (QA) system using Natural Language Processing and a pre-trained Transformer model. The system accepts a paragraph of text as context and answers natural language questions by extracting the most relevant answer from the given context.

## Project Overview
The system is built using the Hugging Face Transformers library and uses the DistilBERT model fine-tuned on the SQuAD dataset. It demonstrates how Transformer-based models can understand questions and retrieve precise answers from unstructured text.

## Technologies Used
- Python
- Hugging Face Transformers
- DistilBERT (distilbert-base-cased-distilled-squad)
- Google Colab
- Jupyter Notebook

## Working Principle
- A context paragraph is provided as input
- A natural language question is asked
- The model processes the question and context together
- The most relevant answer span is returned with a confidence score

## Example
Question: Who is known for the RKO?
Answer: Randy Orton

## How to Run
- Open the notebook in Google Colab
- Run all cells sequentially
- Modify the context and questions to test different queries

Note: Notebook outputs are cleared for GitHub compatibility. Run the notebook in Google Colab to view full results.

## Project Structure
- Question_Answering_NLP.ipynb
- README.md

## Applications
- Question answering chatbots
- Information retrieval systems
- Educational applications
- Customer support systems

## Future Enhancements
- Add a web-based interface
- Support multiple context documents
- Use larger or domain-specific QA models
- Implement voice-based question answering

